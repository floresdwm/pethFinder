{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pet ID 1835 salvo com sucesso.\n",
      "Pet ID 1836 salvo com sucesso.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import Workbook\n",
    "from openpyxl import load_workbook\n",
    "import time\n",
    "\n",
    "def extrair_detalhes_pet(url):\n",
    "    # Configuração do Selenium para usar um driver do Chrome\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Para executar o Chrome em modo headless (sem interface gráfica)\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    # Carregar a página\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Obter o código HTML da página após a renderização completa\n",
    "    page_html = driver.page_source\n",
    "    \n",
    "    # Fechar o driver do Selenium\n",
    "    driver.quit()\n",
    "    \n",
    "    # Parsear o HTML com BeautifulSoup\n",
    "    soup = BeautifulSoup(page_html, 'html.parser')\n",
    "    \n",
    "    # Extrair detalhes do pet\n",
    "    nome_pet = soup.find('div', class_='v-col-md-8').find('span').get_text(strip=True)\n",
    "    \n",
    "    status_pet_tag = soup.find('div', class_='v-chip__content')\n",
    "    status_pet = status_pet_tag.get_text(strip=True) if status_pet_tag else 'Não encontrado'\n",
    "    \n",
    "    descricao_pet_tag = soup.find('div', class_='v-alert__content')\n",
    "    descricao_pet = descricao_pet_tag.get_text(strip=True) if descricao_pet_tag else 'Não encontrado'\n",
    "    \n",
    "    abrigo_presente, nome_abrigo, endereco_abrigo = verificar_abrigo(page_html)\n",
    "    \n",
    "    url_imagem = extrair_url_imagem(url)\n",
    "    \n",
    "    return {'Nome do Pet': nome_pet, 'Status do Pet': status_pet, 'Descrição do Pet': descricao_pet, \n",
    "            'Abrigo': nome_abrigo if abrigo_presente else 'Não encontrado', 'Endereço do Abrigo': endereco_abrigo if abrigo_presente else 'Não encontrado', \n",
    "            'URL': url, 'URL da Imagem': url_imagem}\n",
    "\n",
    "def extrair_url_imagem(url):\n",
    "    # Configuração do Selenium para usar um driver do Chrome\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Para executar o Chrome em modo headless (sem interface gráfica)\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    # Carregar a página\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Obter o código HTML da página após a renderização completa\n",
    "    page_html = driver.page_source\n",
    "    \n",
    "    # Fechar o driver do Selenium\n",
    "    driver.quit()\n",
    "    \n",
    "    # Parsear o HTML com BeautifulSoup\n",
    "    soup = BeautifulSoup(page_html, 'html.parser')\n",
    "    \n",
    "    # Encontrar a tag img com a classe v-img__img e extrair a URL da imagem\n",
    "    img_tag = soup.find('img', class_='v-img__img')\n",
    "    if img_tag:\n",
    "        return img_tag['src']\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def extrair_url_imagem_timer(url):\n",
    "    # Configuração do Selenium para usar um driver do Chrome\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Para executar o Chrome em modo headless (sem interface gráfica)\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    # Carregar a página\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Esperar por um curto período para permitir o carregamento completo da página\n",
    "    time.sleep(4)  # Aguarda 1 segundo (você pode ajustar esse valor conforme necessário)\n",
    "    \n",
    "    # Obter o código HTML da página após a renderização completa\n",
    "    page_html = driver.page_source\n",
    "    \n",
    "    # Fechar o driver do Selenium\n",
    "    driver.quit()\n",
    "    \n",
    "    # Parsear o HTML com BeautifulSoup\n",
    "    soup = BeautifulSoup(page_html, 'html.parser')\n",
    "    \n",
    "    # Encontrar a tag img com a classe v-img__img e extrair a URL da imagem\n",
    "    img_tag = soup.find('img', class_='v-img__img')\n",
    "    if img_tag:\n",
    "        return img_tag['src']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def verificar_abrigo(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    abrigo_info = soup.find('div', class_='v-list-item-title', string=lambda string: string and string.startswith('Abrigo:'))\n",
    "    if abrigo_info:\n",
    "        abrigo = abrigo_info.text.strip().replace('Abrigo:', '').strip()\n",
    "        endereco = abrigo_info.find_next('div', class_='v-list-item-subtitle').text.strip()\n",
    "        return True, abrigo, endereco\n",
    "    \n",
    "    return False, None, None\n",
    "\n",
    "# Cria ou carrega o arquivo Excel\n",
    "try:\n",
    "    wb = load_workbook('databases.xlsx')\n",
    "except FileNotFoundError:\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.append(['Nome do Pet', 'Status do Pet', 'Descrição do Pet', 'Abrigo', 'Endereço do Abrigo', 'URL', 'URL da Imagem'])\n",
    "else:\n",
    "    ws = wb.active\n",
    "\n",
    "# Lista para armazenar os dados de cada pet\n",
    "dados_pets = []\n",
    "\n",
    "# Varrendo os pets 10, 2710\n",
    "for pet_id in range(1835, 1837):\n",
    "    url_pet = f'https://petsrs.com.br/pet/{pet_id}'\n",
    "    try:\n",
    "        detalhes_pet = extrair_detalhes_pet(url_pet)\n",
    "        dados_pets.append(detalhes_pet)\n",
    "        ws.append([detalhes_pet['Nome do Pet'], detalhes_pet['Status do Pet'], detalhes_pet['Descrição do Pet'],\n",
    "                   detalhes_pet['Abrigo'], detalhes_pet['Endereço do Abrigo'], detalhes_pet['URL'], detalhes_pet['URL da Imagem']])\n",
    "        print(f\"Pet ID {pet_id} salvo com sucesso.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar pet ID {pet_id}: {str(e)}\")\n",
    "\n",
    "# Salva as alterações no arquivo Excel\n",
    "wb.save('databases.xlsx')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
